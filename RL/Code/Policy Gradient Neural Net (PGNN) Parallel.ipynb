{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eb0ca3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a39f0b1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "65665d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(IterableDataset):\n",
    "    def __init__(self, env, policy, steps, gamma):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.steps = steps\n",
    "        self.gamma = gamma\n",
    "        self.obs, self.info = env.reset()\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        transitions = []\n",
    "\n",
    "        for step in range(self.steps):\n",
    "            with torch.no_grad():\n",
    "                action = self.policy(torch.as_tensor(self.obs, dtype=torch.float32))\n",
    "            action = action.multinomial(1).cpu().numpy()\n",
    "            next_obs, reward, terminate, truncate, info = self.env.step(action.flatten())\n",
    "            transitions.append((self.obs, action, reward, terminate))\n",
    "            self.obs = next_obs\n",
    "\n",
    "        obs_b, action_b, reward_b, done_b = map(np.stack, zip(*transitions))\n",
    "\n",
    "        running_return = np.zeros(self.env.num_envs, dtype=np.float32)\n",
    "        return_b = np.zeros_like(reward_b)\n",
    "\n",
    "        for row in range(self.steps-1,-1,-1):\n",
    "            running_return = reward_b[row] + (1-done_b[row]) * self.gamma * running_return\n",
    "            return_b[row] = running_return\n",
    "\n",
    "        num_samples = self.env.num_envs * self.steps\n",
    "        obs_b = obs_b.reshape(num_samples, -1)\n",
    "        action_b = action_b.reshape(num_samples, -1)\n",
    "        return_b = return_b.reshape(num_samples, -1)\n",
    "\n",
    "        return_b = (return_b - np.mean(return_b)) / np.std(return_b + 1e-06)\n",
    "\n",
    "        idx = list(range(num_samples))\n",
    "        random.shuffle(idx)\n",
    "\n",
    "        for i in idx:\n",
    "            yield obs_b[i], action_b[i], return_b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "489b8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, output_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, output_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        probs = self.model(x)\n",
    "        return probs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5bd8f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "  if isinstance(m, nn.Conv2d):\n",
    "      nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "      if m.bias is not None:\n",
    "          nn.init.constant_(m.bias.data, 0)\n",
    "  elif isinstance(m, nn.BatchNorm2d):\n",
    "      nn.init.constant_(m.weight.data, 1)\n",
    "      nn.init.constant_(m.bias.data, 0)\n",
    "  elif isinstance(m, nn.Linear):\n",
    "      nn.init.kaiming_uniform_(m.weight.data)\n",
    "      nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e33fcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.vector.make(env_name, num_envs=5, asynchronous=False)\n",
    "\n",
    "obs_dim = env.single_observation_space.shape[0]\n",
    "n_acts = env.single_action_space.n\n",
    "\n",
    "# make core of policy network\n",
    "hidden_sizes = 32\n",
    "logits_net = PolicyNet(obs_dim, hidden_sizes, n_acts)\n",
    "logits_net.apply(initialize_weights)\n",
    "# make optimizer\n",
    "optimizer = AdamW(logits_net.parameters(), lr=0.0003)\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "afe42248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to compute action distribution\n",
    "def get_policy(obs):\n",
    "    probs = logits_net(obs)\n",
    "    return Categorical(probs=probs)\n",
    "\n",
    "\n",
    "# make action selection function (outputs int actions, sampled from policy)\n",
    "def get_action(obs):\n",
    "    return get_policy(obs).sample().item()\n",
    "\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights):\n",
    "    probs = logits_net(obs)\n",
    "    log_probs = torch.log(probs + 1e-6)\n",
    "    action_log_prob = log_probs.gather(1, act)\n",
    "    return -(action_log_prob * weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "140da3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(env, logits_net, 256 , 0.99)\n",
    "loader = DataLoader(data, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fda2702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    for batch in loader:\n",
    "        with torch.no_grad():\n",
    "            batch_obs, batch_acts, batch_weights = batch\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=batch_obs, act=batch_acts, weights=batch_weights)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "faeaa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores=[]\n",
    "scores = []\n",
    "def run_test(trajectories, policy):\n",
    "    scores = deque(maxlen=50)\n",
    "    env2 = gym.make(env_name)\n",
    "    for trajectory in range(trajectories):\n",
    "        trajectory_return = 0\n",
    "        obs, info = env2.reset()\n",
    "        truncate, terminate = False, False\n",
    "        while not terminate and not truncate:\n",
    "            action = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            next_obs, reward, terminate, truncate, _ = env2.step(action)\n",
    "            obs = next_obs\n",
    "            trajectory_return += reward\n",
    "        scores.append(trajectory_return)\n",
    "    final_scores.append(np.mean(scores))\n",
    "    del env2\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "229183d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50  Average Score: -108.02601847984656\n",
      "Episode: 100  Average Score: 5.604392287029273\n",
      "Episode: 150  Average Score: -43.639536650226596\n",
      "Episode: 200  Average Score: 127.23838553842316\n",
      "Episode: 250  Average Score: 95.11367991144292\n",
      "Solved! Episode: 275 Average Score: 197.3313038308102\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    train_one_epoch()\n",
    "    score = run_test(5, logits_net)\n",
    "    if epoch>1 and epoch % 50 == 0:\n",
    "        print(f'Episode: {epoch}  Average Score: {score}' )\n",
    "    if score >= 195:\n",
    "        print(f'Solved! Episode: {epoch} Average Score: {score}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bebd794",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25492\\1223152675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Policy Gradient Neural Net (PGNN) Parallel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Rewards'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(final_scores)),final_scores, marker='.')\n",
    "plt.title('Policy Gradient Neural Net (PGNN) Parallel')\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f29997b9fdfc71aa1dfb5d526be4795f8df3be7b39eddf72af31c014673db1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
